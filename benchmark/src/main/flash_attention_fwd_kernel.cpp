#ifdef C_KERNEL_ENABLE
#include "kernel/flash_attention.h" // Assumes void flash_attention_fwd_cpp(...) is declared
#endif

#ifdef TRITON_KERNEL_ENABLE
#include "flash_attention_fwd_kernel_launcher.h" // Generated by build_kernel.py
#endif

#include "support/support.h"

#include <vector>
#include <string>
#include <cassert>
#include <chrono>
#include <cstring>
#include <iostream>
#include <memory>
#include <random>
#include <stdio.h>
#include <stdlib.h>
#include <cmath> // For sqrtf

using namespace std;
using std::chrono::high_resolution_clock;

// --- Main Test Harness for FlashAttention Forward Pass ---
int main(int argc, char *argv[]) {
    // Default parameters
    int Z = 1;    // Batch Size
    int H = 2;    // Number of Heads
    int N_CTX_Q = 128; // Sequence Length for Q
    int N_CTX_K = 128; // Sequence Length for K/V
    int D_HEAD = 64;   // Dimension of Head
    int RUN_COUNT = 3; // Reduced for faster testing

    // --- Argument Parsing ---
    // Example: "ZxHxN_CTX_QxN_CTX_KxD_HEADxRUN_COUNT" -> "1x2x128x128x64x3"
    if (argc >= 2) {
        std::vector<int> Shape = splitStringToInts(argv[1]);
        if (Shape.size()) {
            assert(Shape.size() == 6 && "Invalid shape: ZxHxN_CTX_QxN_CTX_KxD_HEADxRUN_COUNT\n");
            Z = Shape.at(0); H = Shape.at(1); N_CTX_Q = Shape.at(2);
            N_CTX_K = Shape.at(3); D_HEAD = Shape.at(4); RUN_COUNT = Shape.at(5);
        }
    }
    printf("FlashAttention Fwd Data: Z=%d, H=%d, N_CTX_Q=%d, N_CTX_K=%d, D_HEAD=%d, RUN_COUNT=%d\n",
           Z, H, N_CTX_Q, N_CTX_K, D_HEAD, RUN_COUNT);

    float sm_scale = 1.0f / sqrtf(static_cast<float>(D_HEAD));

    // --- Memory Allocation ---
    size_t q_elements = (size_t)Z * H * N_CTX_Q * D_HEAD;
    size_t k_elements = (size_t)Z * H * N_CTX_K * D_HEAD;
    size_t v_elements = (size_t)Z * H * N_CTX_K * D_HEAD;
    size_t out_elements = (size_t)Z * H * N_CTX_Q * D_HEAD;
    size_t m_logsumexp_elements = (size_t)Z * H * N_CTX_Q;

    float *q_ptr = (float *)malloc(q_elements * sizeof(float));
    float *k_ptr = (float *)malloc(k_elements * sizeof(float));
    float *v_ptr = (float *)malloc(v_elements * sizeof(float));
    float *ref_output_ptr = (float *)malloc(out_elements * sizeof(float));
    float *real_output_ptr = (float *)malloc(out_elements * sizeof(float));
    float *m_logsumexp_unused_ptr = (float *)malloc(m_logsumexp_elements * sizeof(float)); // For Triton kernel if it writes it

    if (!q_ptr || !k_ptr || !v_ptr || !ref_output_ptr || !real_output_ptr || !m_logsumexp_unused_ptr) {
        printf("ERROR: Memory allocation failed!\n");
        // Basic cleanup
        free(q_ptr); free(k_ptr); free(v_ptr); free(ref_output_ptr); free(real_output_ptr); free(m_logsumexp_unused_ptr);
        return -1;
    }
    memset(real_output_ptr, 0, out_elements * sizeof(float));
    memset(m_logsumexp_unused_ptr, 0, m_logsumexp_elements * sizeof(float));


    // --- Data Initialization ---
#ifdef CHECK_ACCURACY
    printf("Mode: CHECK_ACCURACY. Loading data from text files...\n");
    // Define SHAPE strings for getDB. This is an example, adjust to your file naming.
    // Input Q: ZxHxN_CTX_QxD_HEAD, index 1
    // Input K: ZxHxN_CTX_KxD_HEAD, index 2
    // Input V: ZxHxN_CTX_KxD_HEAD, index 3
    // Ref Out: ZxHxN_CTX_QxD_HEAD, index 4
    // For simplicity, getDB might take a flattened shape like "TOTAL_ELEMENTS"
    // Or we adapt readMatrix to handle multi-dim conceptually.
    // Let's assume files store flattened data and readMatrix reads M=1, N=TOTAL_ELEMENTS

    std::string q_shape_str = std::to_string(Z*H*N_CTX_Q*D_HEAD); // Example: flattened
    std::string k_shape_str = std::to_string(Z*H*N_CTX_K*D_HEAD);
    std::string v_shape_str = std::to_string(Z*H*N_CTX_K*D_HEAD);
    std::string out_shape_str = std::to_string(Z*H*N_CTX_Q*D_HEAD);
    int dummy_m, dummy_n; // For readMatrix

    if (!readLoss(getDB("flash_attention", q_shape_str, 1).c_str(), q_ptr, q_elements)) return -1; // Using readLoss for 1D float array
    printf("Q loaded.\n");
    if (!readLoss(getDB("flash_attention", k_shape_str, 2).c_str(), k_ptr, k_elements)) return -1;
    printf("K loaded.\n");
    if (!readLoss(getDB("flash_attention", v_shape_str, 3).c_str(), v_ptr, v_elements)) return -1;
    printf("V loaded.\n");
    if (!readLoss(getDB("flash_attention", out_shape_str, 4).c_str(), ref_output_ptr, out_elements)) return -1;
    printf("Ref Output loaded.\n");

#else
    printf("Mode: Generating random data (CHECK_ACCURACY not defined)...\n");
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution<float> ur_dist(-1.0f, 1.0f);

    for (size_t i = 0; i < q_elements; ++i) q_ptr[i] = ur_dist(gen);
    for (size_t i = 0; i < k_elements; ++i) k_ptr[i] = ur_dist(gen);
    for (size_t i = 0; i < v_elements; ++i) v_ptr[i] = ur_dist(gen);
    printf("Random data generated.\n");
#endif

    // --- Kernel Execution and Timing ---
    // Block sizes for C++ version (passed to function)
    // For Triton, these are part of kernel compilation config (config.json)
    int CPP_BLOCK_M = 64; 
    int CPP_BLOCK_N = 64;
    if (D_HEAD > 128) { CPP_BLOCK_M = 32; CPP_BLOCK_N = 32; }
    else if (D_HEAD <=32) { CPP_BLOCK_M = 128; CPP_BLOCK_N = 128; }


#ifdef C_KERNEL_ENABLE
    printf("Executing C FlashAttention Fwd Kernel %d times...\n", RUN_COUNT);
    auto c_fwd_begin_time = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < RUN_COUNT; i++) {
        flash_attention_fwd_cpp(q_ptr, k_ptr, v_ptr, real_output_ptr, m_logsumexp_unused_ptr,
                                Z, H, N_CTX_Q, N_CTX_K, D_HEAD,
                                sm_scale, CPP_BLOCK_M, CPP_BLOCK_N, (bool)(std::getenv("CAUSAL") ? std::stoi(std::getenv("CAUSAL")) : 0));
    }
    auto c_fwd_end_time = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> c_fwd_time_interval = c_fwd_end_time - c_fwd_begin_time;
    PRINT_KERNEL_RUNNING_TIME(C_KERNEL, c_fwd_time_interval.count() / RUN_COUNT)
#endif

#ifdef TRITON_KERNEL_ENABLE
    printf("Executing Triton FlashAttention Fwd Kernel %d times...\n", RUN_COUNT);
    // Grid: (num_q_blocks, num_batch_heads)
    // BLOCK_M, BLOCK_N, BLOCK_DMODEL are taken from config.json for Triton kernel compilation
    // Here we use the runtime dimensions to calculate grid.
    // The BLOCK_M used for grid calculation should match the one used in config.json for flash_attention.py
    // Example: This should match config.json's blk[0] for flash_attention.py
                                // Or read from an env var set by config.json if possible
    // char* blk_m_str = std::getenv("TRITON_BLK_M"); // Hypothetical
    // if (blk_m_str) TRITON_CFG_BLOCK_M = std::stoi(blk_m_str);


    int grid_dim0 = (N_CTX_Q + flash_attention_fwd_kernel_BLOCK_M - 1) / flash_attention_fwd_kernel_BLOCK_M; // cdiv
    int grid_dim1 = Z * H;
    int num_threads_triton = 1; // Example

    // Strides for Triton kernel (assuming contiguous Z, H, SeqLen, HeadDim)
    long stride_qz_tr = H * N_CTX_Q * D_HEAD, stride_qh_tr = N_CTX_Q * D_HEAD, stride_qm_tr = D_HEAD, stride_qk_tr = 1;
    long stride_kz_tr = H * N_CTX_K * D_HEAD, stride_kh_tr = N_CTX_K * D_HEAD, stride_km_tr = D_HEAD, stride_kk_tr = 1;
    long stride_vz_tr = H * N_CTX_K * D_HEAD, stride_vh_tr = N_CTX_K * D_HEAD, stride_vm_tr = D_HEAD, stride_vk_tr = 1;
    long stride_oz_tr = H * N_CTX_Q * D_HEAD, stride_oh_tr = N_CTX_Q * D_HEAD, stride_om_tr = D_HEAD, stride_ok_tr = 1;
    // bool is_causal_runtime = (bool)(std::getenv("CAUSAL") ? std::stoi(std::getenv("CAUSAL")) : 0);


    auto triton_fwd_begin_time = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < RUN_COUNT; i++) {
        // IMPORTANT: Verify this signature against the generated flash_attention_kernel_launcher.h
        // The order and number of scalar args after tensor pointers is critical.
        // It's likely Z, H, N_CTX_Q, N_CTX_K, D_HEAD, sm_scale. IS_CAUSAL is constexpr in kernel.
        // The kernel has IS_CAUSAL as constexpr, so it's compiled in.
        // The launcher might not pass it. We need to check.
        // If IS_CAUSAL is truly constexpr, then we need two compiled versions of the kernel
        // or the launcher has a way to select. Let's assume the wrapper takes all runtime dims.
        flash_attention_fwd_kernel_wrap(
            grid_dim0, grid_dim1, 1,  // gridX, gridY, gridZ
            num_threads_triton,       // num_threads
            flash_attention_fwd_kernel, // kernel_ptr_t
            q_ptr, k_ptr, v_ptr, real_output_ptr, m_logsumexp_unused_ptr, // Tensor args
            // Scalar args (must match generated launcher)
            (int32_t)stride_qz_tr, (int32_t)stride_qh_tr, (int32_t)stride_qm_tr,
            (int32_t)stride_kz_tr, (int32_t)stride_kh_tr, (int32_t)stride_km_tr,
            (int32_t)stride_vz_tr, (int32_t)stride_vh_tr, (int32_t)stride_vm_tr,
            (int32_t)stride_oz_tr, (int32_t)stride_oh_tr, (int32_t)stride_om_tr,
            (int32_t)H, (int32_t)N_CTX_Q, (int32_t)D_HEAD, sm_scale
            // IS_CAUSAL is tl.constexpr in kernel, so it's compiled in.
            // The wrapper *might* not take it. Check generated header!
            // If it does, add: (int32_t)is_causal_runtime
        );
    }
    auto triton_fwd_end_time = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> triton_fwd_time_interval = triton_fwd_end_time - triton_fwd_begin_time;
    PRINT_KERNEL_RUNNING_TIME(TRITON_KERNEL, triton_fwd_time_interval.count() / RUN_COUNT)
#endif

    // --- Accuracy Check ---
#ifdef CHECK_ACCURACY
    printf("Checking accuracy for FlashAttention Fwd...\n");
    check_tensor(ref_output_ptr, real_output_ptr, out_elements, "flash_attention_fwd_output");
    printf("Accuracy check complete.\n");
#endif

    // --- Save Test Data ---
#ifdef KEEP_TEST_DATA
    // ... (Similar saving logic as other kernels, adapt for Q, K, V, real_output_ptr) ...
    printf("Mode: KEEP_TEST_DATA. Saving data for FlashAttention (not fully implemented here)...\n");
#endif

    // --- Cleanup ---
    printf("Cleaning up FlashAttention Fwd memory...\n");
    free(q_ptr); free(k_ptr); free(v_ptr);
    free(ref_output_ptr); free(real_output_ptr); free(m_logsumexp_unused_ptr);
    printf("Cleanup complete. Exiting FlashAttention Fwd main.\n");

    return 0;
}